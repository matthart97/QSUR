{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import pickle\n",
    "import pickle\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary) and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to embed descriptions using BERT\n",
    "def get_bert_embeddings(texts):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    embeddings = []\n",
    "    with torch.no_grad():  # No need to compute gradients\n",
    "        for text in texts:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            outputs = model(**inputs)\n",
    "            # Get the embeddings from the last hidden state (mean pooling)\n",
    "            embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to load embeddings from a file\n",
    "def load_embeddings(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    return embeddings\n",
    "\n",
    "# Function to compute cosine similarity between two embeddings\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    # Cosine similarity is the inverse of cosine distance\n",
    "    return 1 - cosine(embedding1, embedding2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1034390/1524026631.py:3: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('/home/matt/Proj/QSURv3/Data/Curated/UseCaseDataModeling.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../Data/Curated/UseCaseDataModeling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = df['Harmonized Functional Use'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_bert_embeddings_with_labels(texts, labels):\n",
    "    model.eval()\n",
    "    embeddings_dict = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text, label in zip(texts, labels):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            outputs = model(**inputs)\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "            embeddings_dict[label] = embedding\n",
    "    \n",
    "    return embeddings_dict\n",
    "\n",
    "# Create DataFrame from embeddings\n",
    "def create_embeddings_df(embeddings_dict):\n",
    "    # Convert dictionary to DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'label': list(embeddings_dict.keys()),\n",
    "        'embedding': list(embeddings_dict.values())\n",
    "    })\n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "labels = functions\n",
    "embeddings_dict = get_bert_embeddings_with_labels(functions, labels)\n",
    "\n",
    "# Create DataFrame\n",
    "df_embeddings = create_embeddings_df(embeddings_dict)\n",
    "\n",
    "# Save to file (multiple options)\n",
    "# 1. Save as pickle\n",
    "df_embeddings.to_pickle('FunctionEmbeddings.pkl')\n",
    "\n",
    "# 2. Save as numpy array with labels\n",
    "np.savez('FunctionEmbeddings.npz', \n",
    "         embeddings=np.array([e for e in embeddings_dict.values()]), \n",
    "         labels=np.array(list(embeddings_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Solvent</td>\n",
       "      <td>[0.102317356, 0.117060356, -0.06408139, 0.1943...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Surfactant (surface active agent)</td>\n",
       "      <td>[-0.11199236, 0.025828337, 0.14529149, -0.1922...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Humectant</td>\n",
       "      <td>[0.18350892, -0.39270502, -0.20201614, 0.08241...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Binder</td>\n",
       "      <td>[-0.15956144, -0.32352757, -0.15509589, -0.122...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fragrance</td>\n",
       "      <td>[0.20288669, -0.03972793, -0.122679375, -0.163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Alloying element</td>\n",
       "      <td>[0.014706743, -0.05031349, -0.43574685, 0.1439...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Anti-streaking agent</td>\n",
       "      <td>[-0.38540557, -0.46419767, -0.4762463, -0.2515...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Color scavenger (EPA)</td>\n",
       "      <td>[-0.01568828, -0.10239726, -0.45295116, -0.093...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Flocculating agent</td>\n",
       "      <td>[-0.08261341, -0.25251776, -0.0062987506, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Tracer</td>\n",
       "      <td>[0.006890714, -0.22597751, -0.30322284, -0.043...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                label  \\\n",
       "0                             Solvent   \n",
       "1   Surfactant (surface active agent)   \n",
       "2                           Humectant   \n",
       "3                              Binder   \n",
       "4                           Fragrance   \n",
       "..                                ...   \n",
       "90                   Alloying element   \n",
       "91               Anti-streaking agent   \n",
       "92              Color scavenger (EPA)   \n",
       "93                 Flocculating agent   \n",
       "94                             Tracer   \n",
       "\n",
       "                                            embedding  \n",
       "0   [0.102317356, 0.117060356, -0.06408139, 0.1943...  \n",
       "1   [-0.11199236, 0.025828337, 0.14529149, -0.1922...  \n",
       "2   [0.18350892, -0.39270502, -0.20201614, 0.08241...  \n",
       "3   [-0.15956144, -0.32352757, -0.15509589, -0.122...  \n",
       "4   [0.20288669, -0.03972793, -0.122679375, -0.163...  \n",
       "..                                                ...  \n",
       "90  [0.014706743, -0.05031349, -0.43574685, 0.1439...  \n",
       "91  [-0.38540557, -0.46419767, -0.4762463, -0.2515...  \n",
       "92  [-0.01568828, -0.10239726, -0.45295116, -0.093...  \n",
       "93  [-0.08261341, -0.25251776, -0.0062987506, -0.0...  \n",
       "94  [0.006890714, -0.22597751, -0.30322284, -0.043...  \n",
       "\n",
       "[95 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QSUR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
